---
title: "Isotonic regression"
author:
  - name:
      given: Joseph
      family: Salmon

date: "2024-10-22"
description: "Iso Iso Iso ... Tonic"
categories: [statistics, optimization, machine learning]
image: "isotonic.png"
reference-location: document
fig-cap-location: bottom
draft: false

# title-block-banner: hello-world-bg.jpg

format:
  html:
    include-before-body: ../../html/margin_image.html
    include-after-body: ../../html/blog_footer.html
    code-fold: true
    highlight-style: github 
---


::: {.content-visible when-format="html"}
::: {.hidden}
{{
\usepackage{amssymb, amsmath, amsthm}
\usepackage{stmaryrd}
\DeclareMathOperator*{\argmin}{arg\,min} % thin space, limits underneath in displays
\DeclareMathOperator*{\argmax}{arg\,max} % thin space, limits underneath in displays
\DeclareMathOperator{\diag}{diag}
}}
:::
:::

**[Note]{.underline}**: This blog is mainly inspired by the post by Fabian Pedregosa on [Isotonic regression](https://fabianp.net/blog/isotonic-regression/), and the code use is an adaptation of the PAVA algorithm coded in the `sklearn` library (in particular the `_isotonic.pyx` file, as available in October 2024, see [source](https://github.com/scikit-learn/scikit-learn/blob/04fbe04fedda0e86e67867854900a49fb53c4c01/sklearn/_isotonic.pyx)).

# Introduction

Isotonic regression is a technique used to fit a non-decreasing function to a set of data points.
We can think of isotonic regression as a generalization of linear regression where the function is constrained to be non-decreasing.

More formally suppose that one collects $n$ sample points $y_1,\dots, y_n$ and has weights $w_1, \dots, w_n$ associated with each data point.

**[Note]{.underline}**: when no a priori weights are given, we can set $w_i = 1$ for all $i \in \llbracket 1, n \rrbracket$.

Then, the isotonic regression problem can be formulated as follows:
$$
\begin{align}
\min_{x \in \mathbb{R}^n}
\frac{1}{2}
& \sum_{i=1}^{n} w_i (x_i - y_i)^2 \\
\text{s.t.} & \quad x_1 \leq x_2 \leq \ldots \leq x_n \nonumber
\end{align}
$$ {#eq-isotonic}




```{python}
# |echo: false
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots


def isotonic_regression(z, w):
    y = z.copy()
    n=len(y)
    i = 0
    target = np.arange(len(y))
    targets = [target.copy()]
    idx = [0]
    while i < n:
        k = target[i] + 1
        if k == n:
            break
        if y[i] < y[k]:
            # We are in an increasing subsequence.
            i = k
            targets.append(target.copy())
            idx.append(k)
            continue
        sum_wy = w[i] * y[i]
        sum_w = w[i]
        while True:
            # We are within a decreasing subsequence.
            prev_y = y[k]
            sum_wy += w[k] * y[k]
            sum_w += w[k]
            k = target[k] + 1
            if k == n or prev_y < y[k]:
                targets.append(target.copy())
                idx.append(k)

                # Non-singleton decreasing subsequence is finished,
                # update first entry.
                y[i] = sum_wy / sum_w
                w[i] = sum_w

                target[i] = k - 1
                target[k - 1] = i

                if i > 0:
                    # Backtrack if we can.  This makes the algorithm
                    # single-pass and ensures O(n) complexity.
                    i = target[i - 1]
                # Otherwise, restart from the same point.
                break

    # Reconstruct the solution.
    i = 0
    while i < n:
        k = target[i] + 1
        y[i + 1 : k] = y[i]
        i = k
    return y, targets, idx

n_sample = 25

rng = np.random.default_rng(1)

x = np.arange(n_sample)
w = np.ones(n_sample)
z = rng.integers(-10, 50, size=(n_sample,)) + 50. * np.log(1 + np.arange(n_sample))


def replace_after_first_non_increasing(vector):
    n = len(vector)
    for i in range(1, n):
        if vector[i] < vector[i - 1]:
            mean_value = np.mean(vector[i:])
            vector[i:] = mean_value
            break
    return vector


sol, targets, idx = isotonic_regression(z, w)
primals, duals = [], []

import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots

primals, duals = [], []

def plot_targets(y, targets, x):
    n = len(y)

    # Create subplots
    fig = make_subplots(rows=2, cols=1, subplot_titles=("PAVA output evolution", "Primal and dual evolution"))

    # Initial plot
    fig.add_trace(go.Scatter(x=x, y=y, mode='markers', name='Raw data', marker=dict(color='black', opacity=1, size=10), legendgroup='1', showlegend=True), row=1, col=1)
    fig.add_trace(go.Scatter(x=[x[0]], y=[y[0]], mode='lines+markers', name='PAVA iterations', marker=dict(color='red'), legendgroup='1', showlegend=True), row=1, col=1)
    fig.add_trace(go.Scatter(x=[0], y=[0], mode='lines+markers', name='Duals', marker=dict(color='green'), legendgroup='2', showlegend=True), row=2, col=1)

    yy = y.copy()
    i = 0
    while i < n:
        k = targets[0][i] + 1
        yy[i:k] = np.mean(y[i:k])
        i = k
    primal = replace_after_first_non_increasing(yy.copy())

    fig.add_trace(go.Scatter(x=[0], y=[0.5 * np.linalg.norm(primal - y)**2], mode='lines+markers', name='Primals', marker=dict(color='blue'), legendgroup='2', showlegend=True), row=2, col=1)

    fig.update_layout(
    legend_tracegroupgap = 180,
    )

    frames = []
    for iteration, target in enumerate(targets):
        yy = y.copy()
        i = 0
        while i < n:
            k = target[i] + 1
            yy[i:k] = np.mean(y[i:k])
            i = k
        duals.append(0.5 * np.linalg.norm(y)**2 - 0.5 * np.linalg.norm(yy)**2)

        current_idx = idx[iteration]
        primal = replace_after_first_non_increasing(yy.copy())
        primals.append(0.5 * np.linalg.norm(primal - y)**2)
        frames.append(go.Frame(data=[
            go.Scatter(x=x, y=y, mode='markers', marker=dict(color='black', opacity=1), legendgroup='1', showlegend=True),
            go.Scatter(x=x[:current_idx+1], y=yy[:current_idx+1], mode='lines+markers', marker=dict(color='red'), legendgroup='1', showlegend=True),
            go.Scatter(x=np.arange(iteration+1), y=duals[:iteration+1], mode='lines+markers', marker=dict(color='green'), legendgroup='2', showlegend=True),
            go.Scatter(x=np.arange(iteration+1), y=primals[:iteration+1], mode='lines+markers', marker=dict(color='blue'), legendgroup='2', showlegend=True)
        ], name=str(iteration)))

    frames.append(go.Frame(data=[
        go.Scatter(x=x, y=y, mode='markers', marker=dict(color='black', opacity=1), legendgroup='1', showlegend=True),
        go.Scatter(x=x, y=sol, mode='lines+markers', marker=dict(color='red'), legendgroup='1', showlegend=True),
        go.Scatter(x=np.arange(len(duals)), y=duals, mode='lines+markers', marker=dict(color='green'), legendgroup='2', showlegend=True),
        go.Scatter(x=np.arange(len(primals)), y=primals, mode='lines+markers', marker=dict(color='blue'), legendgroup='2', showlegend=True)
    ], name=str(iteration)))

    fig.frames = frames

    steps = [dict(method="animate", args=[[str(i)], dict(mode="immediate", frame=dict(duration=300, redraw=True), transition=dict(duration=0))], label=str(i)) for i in range(len(frames))]
    sliders = [dict(active=1, steps=steps, currentvalue={"prefix": "Iteration: #"})]

    fig.update_layout(
        sliders=sliders,
        updatemenus=[{
            "buttons": [
                {"args": [None, {"frame": {"duration": 500, "redraw": False}, "fromcurrent": True, "transition": {"duration": 50, "easing": "linear"}}], "label": "Play", "method": "animate"},
                {"args": [[None], {"frame": {"duration": 0, "redraw": False}, "mode": "immediate", "transition": {"duration": 0}}], "label": "Pause", "method": "animate"}
            ],
            "direction": "left", "pad": {"r": 5, "t": 5, "b": 20}, "showactive": False, "type": "buttons", "x": 0.5, "xanchor": "center", "y": -0.28, "yanchor": "top"
        }],
    xaxis_title="X", yaxis_title="Y", template='simple_white', xaxis=dict(range=[-0.5, n+0.5]), yaxis=dict(range=[0, 250]), xaxis2=dict(range=[0, len(targets)]), yaxis2=dict(range=[0, 12000]),
    )

    fig.show()

plot_targets(z, targets, x)
```

Below you will find a simple version of the PAVA algorithm coded in Python. The code illustrated above (the `sklearn` version) is more efficient but less readable.


```{python}
#| echo: true
#| code-fold: false
import numpy as np


def pool_adjacent_violators(y, w):
    """
    Ensure that the sequence y is non-decreasing by pooling adjacent violators.

    Parameters:
    y (list of float): The initial sequence of values.
    w (list of float): The weights associated with each value in y.

    Returns:
    list of float: The adjusted sequence y that is non-decreasing.
    list of float: The adjusted weights w.
    """
    n = len(y)
    r = y.copy()
    gps = [1] * n
    ws = w.copy()

    i = 1
    while i < n:
        if r[i] < r[i - 1]:  # Find adjacent violators
            # Pool the violators
            r[i] = (ws[i] * r[i] + ws[i - 1] * r[i - 1]) / (ws[i] + ws[i - 1])
            ws[i] += ws[i - 1]
            gps[i]+= gps[i - 1]
            # Remove the previous element and decrease the number of elements
            r.pop(i - 1)
            ws.pop(i - 1)
            gps.pop(i - 1)
            n -= 1
            # Move back one step if possible
            if i > 1:
                i -= 1
        else:
            i += 1
    sol = []
    for i, val in enumerate(gps):
        for _ in range(val):
            sol.append(r[i])
    return sol, gps, ws

# n_sample = 25

# rng = np.random.default_rng(1)

# x = np.arange(n_sample)
# w = np.ones(n_sample)
# z = rng.integers(-10, 50, size=(n_sample,)) + 50. * np.log(1 + np.arange(n_sample))
# w = [1] * len(z)  # Uniform weights
# adjusted_y, sol, adjusted_w = pool_adjacent_violators(z.tolist(), w)
# print("Adjusted y:", adjusted_y)
# print("Adjusted weights:", adjusted_w)

```

## Optimization, duality, etc.

The formualtion of the isotonic regression problem given in @eq-isotonic is a quadratic program (QP) with linear constraints.
Using a matrix notation, introducing the matrix
$$A = \begin{pmatrix} 1 & -1 &                       &                       &    \\
  & 1  & -1                    &                       &    \\
  &    & \ddots & \ddots &    \\
  &    &                       & 1                     & -1\end{pmatrix}
 =
\begin{pmatrix}
a_1^\top \\
\vdots   \\
a_{n-1}^\top
\end{pmatrix}
 \in \mathbb{R}^{n-1 \times n}\enspace,
$$
the isotonic regression problem can be formulated as follows:
$$
\begin{align}
\min_{x \in \mathbb{R}^n}
\frac{1}{2}
& \sum_{i=1}^{n} w_i (x_i - y_i)^2 \\
\text{s.t.} & \quad Ax \leq 0 \nonumber
\end{align}
$$ {#eq-isotonic-matrix}



In what follows we will write $W=\diag(w_1,\dots,w_n)$, assuming that $w_i > 0$ for all $i \in \llbracket 1, n \rrbracket$, so $W^{-1}= \diag(1/w_1,\dots,1/w_n)$.
Hence, we can rewrite the isotonic regression problem as

$$
\begin{align}
\min_{x \in \mathbb{R}^n}
\frac{1}{2}
& (y-x)^\top W (y-x) \\
\text{s.t.} & \quad Ax \leq 0 \enspace.
\end{align}
$$ {#eq-isotonic-matrix-2}

Hence, the isotonic regression problem can be formulated as a convex quadratic program with linear constraints.

Note in particular that the set of **isotonic vectors** $\mathcal{K} = \left\{ x \in \mathbb{R}^n : Ax \leq 0 \right\}$
is a cone (stable by positive scalar mutliplication).
We remind also that the **polar cone** of $\mathcal{K}$ is $\mathcal{K}^\circ= \left\{v \in \mathbb{R}^n : \langle v, x \rangle \leq 0 \text{ for all } x \in \mathcal{K}\right\}$.


::: {#lem-polar}

## Polar cone of the isotonic cone
The polar cone of $\mathcal{K}$ is
$$
\mathcal{K}^\circ =\left\{ \sum_{i=1}^{n-1} \alpha_i a_i, \text{ for } \alpha_1 \geq 0, \dots, \alpha_{n-1} \geq 0 \right\} = A^\top \mathbb{R}_+^{n-1}\enspace.
$$

:::

::: {.proof}
Let $x \in \mathcal{K}$ and $v = \sum_{i=1}^{n-1} \alpha_i a_i$ with $\alpha_1 \geq 0, \dots, \alpha_{n-1} \geq 0$.
Then, note that $A x \leq 0$ is equivalent to $\langle a_i, x \rangle \leq 0$ for all $i \in \llbracket 1, n-1 \rrbracket$. Hence, we have
$$
\begin{align*}
\langle v, x \rangle
& = \sum_{i=1}^{n-1} \alpha_i \langle a_i, x \rangle\\
& \leq 0 \enspace.
\end{align*}
$$
So $\left\{ \sum_{i=1}^{n-1} \alpha_i a_i, \text{ with } \alpha_1 \geq 0, \dots, \alpha_{n-1} \geq 0 \right\} \subset \mathcal{K}^\circ$.

For the converse, let $v \in \mathcal{K}^\circ$.
One can check that $(a_i)_{i=1,\dots,n-1}$ are linearly independant, so adding the vector $\mathbf{1}_n=(1,\dots,1)^\top/n$ create a basis of $\mathbb{R}^n$.

Hence one can write $v= \alpha_n \mathbf{1}_n + \sum_{i=1}^{n-1} \alpha_i a_i$.
Now choosing $x=\mathbf{1}_n \in \mathcal{K}$ yields $\langle v, \mathbf{1}_n \rangle = \alpha_n \leq 0$. Choosing $x = - \mathbf{1}_n \in \mathcal{K}$ yields $\langle v, - \mathbf{1}_n \rangle = - \alpha_n \leq 0$. Hence, $\alpha_n = 0$.

Now, for all $i \in \llbracket 1, n-1 \rrbracket$, choosing $x = \sum_{k=i}^n e_i$ for $i \in \llbracket 2,n\rrbracket$ yields $\langle v, x \rangle = -\alpha_{i}\leq 0$. Hence, for all $ i\in \llbracket 1, n-1 \rrbracket$, we have $\alpha_i \geq 0$, so $\mathcal{K}^\circ \subset \left\{ \sum_{i=1}^{n-1} \alpha_i a_i, \text{ with } \alpha_1 \geq 0, \dots, \alpha_{n-1} \geq 0 \right\}$ and the lemma is proved.

:::

### Fenchel transform

Here we remind the definition of the Fenchel transform of a function $f$ defined on $\mathbb{R}^n$:
$$
f^*(v) = \sup_{x \in \mathbb{R}^n} \left\{ \langle v, x \rangle - f(x) \right\}\enspace.
$$

We also write $\iota_{\mathcal{K}}$ the indicator function of the set $\mathcal{K}$, that is $\iota_{\mathcal{K}}(x) = 0$ if $x \in \mathcal{K}$ and $+\infty$ otherwise.

::: {#lem-fenchel}

## Fenchel transform of indicator of a cone
The Fenchel transform of the indicator function of a convex cone $\mathcal{K}$ is
$$
\iota_{\mathcal{K}}^*(v) = \iota_{\mathcal{K}^\circ}(v)\enspace,
$$
where $\iota_{\mathcal{K}^\circ}$ is the indicator function of the set $\mathcal{K}^\circ$.

:::


### Dual problem

Let us now derive the dual problem of the isotonic regression problem given in @eq-isotonic-matrix.

::: {#thm-dual}

## Dual problem of isotonic regression
The dual problem of the isotonic regression problem given in @eq-isotonic-matrix is
$$
\begin{align*}
\max_{\alpha \in \mathbb{R}_+^{n-1}} &
\left[
    - \frac{1}{2} (A^\top \alpha-W y)^\top W^{-1} (A^\top \alpha-W y)
    + \frac{1}{2} y^\top W y
\right] \enspace.
\end{align*}
$$

:::

::: {.proof}

With such ingredient we can write the isotonic regression problem as:
$$
\begin{align}
\min_{x \in \mathbb{R}^n}
\frac{1}{2}
(y-x)^\top W (y-x) + \iota_{\mathcal{K}}(x) \enspace.
\end{align}
$$ {#eq-isotonic-matrix-2}
We can now rewrite the formulation as

$$
\begin{align*}
\min_{x \in \mathbb{R}^n} &
\frac{1}{2}
z^\top W z + \iota_{\mathcal{K}}(x)\\
\text{s.t.} & \quad z = y - x  \enspace.
\end{align*}
$$
We can now introduce the Lagrangian of the problem:
$$
\begin{align*}
\mathcal{L}(x, z, \lambda) & = \frac{1}{2} z^\top W z + \iota_{\mathcal{K}}(x) + \lambda^\top (y - z - x)\\
& = \frac{1}{2} z^\top W z + \iota_{\mathcal{K}}(x) + \lambda^\top y - \lambda^\top z - \lambda^\top x \enspace.
\end{align*}
$$
Assuming strong duality holds, we can write the dual problem as
$$
\begin{align*}
\min_{x \in \mathbb{R}^n, z \in \mathbb{R}^n} \max_{\lambda \in \mathbb{R}^n} ~
\mathcal{L}(x, z, \lambda) =
\max_{\lambda \in \mathbb{R}^n} \min_{x \in \mathbb{R}^n, z \in \mathbb{R}^n} &
\mathcal{L}(x, z, \lambda) \enspace.
\end{align*}
$$
Now, one can check that the dual problem is equivalent to

$$
\max_{\lambda \in \mathbb{R}^n} \min_{x \in \mathbb{R}^n, z \in \mathbb{R}^n} \left\{ \frac{1}{2} z^\top W z + \iota_{\mathcal{K}}(x) + \lambda^\top y - \lambda^\top z - \lambda^\top x \right\} \enspace.
$$
Separating the terms in $x$ and $z$ yields
$$
\max_{\lambda \in \mathbb{R}^n}
\left[
    \min_{x \in \mathbb{R}^n} \left\{ \iota_{\mathcal{K}}(x) - \lambda^\top x + \lambda^\top y \right\} + \min_{z \in \mathbb{R}^n} \left\{ \frac{1}{2} z^\top W z - \lambda^\top z \right\}
\right]
\enspace.
$$
The second term is a simple quadratic problem with a unique solution given by $z = W^{-1} \lambda$, hence this can be rewritten as
$$
\left[
- \frac{1}{2} \lambda^\top W^{-1} \lambda
\right]
\enspace.
$$
The first term is linked to the Fenchel transform of the indicator function of the cone $\mathcal{K}$, and reads
$$
\left[
    - \iota_{\mathcal{K}^\circ}(\lambda) + \lambda^\top y = - \iota_{\mathcal{K}^\circ}(\lambda) + \lambda^\top W^{-1} W y
\right]
\enspace.
$$

Hence the dual problem reads:
$$
\begin{align*}
\max_{\lambda \in \mathbb{R}^n} &
\left[
    - \iota_{\mathcal{K}^\circ}(\lambda) + \lambda^\top y - \frac{1}{2} \lambda^\top W^{-1} \lambda
\right] \enspace,
\end{align*}
$$
the later can be rewritten as

$$
\begin{align*}
\max_{\lambda \in \mathcal{K}^{\circ}} &
\left[
    - \frac{1}{2} (\lambda-W y)^\top W^{-1} (\lambda-W y)
    + \frac{1}{2} y^\top W y
\right] \enspace.
\end{align*}
$$
Eventually, using the formulation of $\mathcal{K}^{\circ}$ given in @lem-polar, the dual problem can be rewritten as
$$
\begin{align*}
\max_{\alpha \in \mathbb{R}_+^{n-1}} &
\left[
    - \frac{1}{2} (A^\top \alpha-W y)^\top W^{-1} (A^\top \alpha-W y)
    + \frac{1}{2} y^\top W y
\right] \enspace,
\end{align*}
$$
which is the targeted formulation.

:::

Now given a primal feasible point $x \in \mathcal{K}$ and a dual feasible point $\alpha \in \mathbb{R}_+^{n-1}$, we can obtain upper and lower bounds on the optimal value of the primal problem.

We consider here the case where $W=I_n$ (uniform weights). The iterates of the PAVA algorithm will be dual feasible and can represent $y-\lambda$.
For creating a primal feasible point, one can simply take the ordered version of the output, making the last element constant to create a non-decreasing vector.



### Karush-Kuhn-Tucker (KKT) conditions

The KKT conditions for the isotonic regression problem are given by

::: {#thm-kkt}

## KKT conditions for isotonic regression
Let $x^{\star} \in \mathcal{K}$ and $\lambda^{\star} \in \mathbb{R}_+^{n-1}$ be primal and dual optimal solutions, then the KKT conditions for the isotonic regression problem reads
$$
\begin{align*}
W (y-x^{\star}) + A^\top \lambda^{\star} & = 0 & ~ (\textbf{stationarity})\\
\langle \lambda^{\star} , A x^{\star} \rangle & = 0 & ~ (\textbf{complementary slackness})\\
\lambda^{\star} & \geq 0 & ~ (\textbf{dual feasibility})\\
A x^{\star} & \leq 0 & ~  (\textbf{primal feasibility})
\end{align*}
$$

:::

::: {.proof}
See [@Boyd_Vandenberghe04, Sec. 5.5.3 ]
:::

In this convex setting (a primal strictly feasible point exists, say $x=(1,\dots,n)^\top$, so the Slater condition holds), the KKT conditions are necessary and sufficient for optimality.

**Stationarity** can be rephrased as:

$$
\begin{align*}
w_1 (y_1 - x^{\star}_1) - \lambda^{\star}_1 & = 0 \\
w_i (y_i - x^{\star}_i) + \lambda^{\star}_{i-1} - \lambda^{\star}_i & = 0, \quad \text{for all } i \in \llbracket 2, n-1 \rrbracket\\
w_n (y_n - x^{\star}_n) + \lambda^{\star}_{n-1} & = 0 \enspace.
\end{align*}
$$
With the convention $\lambda_0^{\star} = \lambda_n^{\star} = 0$, this reduces to 
$$
\begin{align*}
w_i (y_i - x^{\star}_i) + \lambda^{\star}_{i-1} - \lambda^{\star}_i & = 0, \quad \text{for all } i \in \llbracket 1, n \rrbracket\\
\end{align*}
$$ {#eq-stationarity}



**Complementarity slackness** can also be rephrased:
$$
\begin{align*}
& \lambda_i^{\star} (A x^{\star})_i  = 0 \quad \text{for all } i \in \llbracket 1, n-1 \rrbracket \\
\iff & \lambda_i^{\star} (x^{\star}_i-x^{\star}_{i+1})  = 0 \quad \text{for all } i \in \llbracket 1, n-1 \rrbracket \enspace.
\end{align*}
$$


A simple consequence of the complementarity slackness is as follows: if $\lambda_i^{\star} > 0$ for some $i \in \llbracket 1, n-1 \rrbracket$, then $x^{\star}_i = x^{\star}_{i+1}$.
Hence, the set of ($n-1$) constraints can be partitioned into contiguous blocks.
Each block B can be written as $B = \llbracket \underline{b}, \overline{b} \rrbracket$ for some $\underline{b} \leq \overline{b}$ and $x^{\star}_{\underline{b}} = x^{\star}_{\underline{b}+1} = \ldots = x^{\star}_{\overline{b}}$. When needed we use the convention $x^{\star}_{0}=0$ and $x^{\star}_{n+1}=n+1$. and $x^{\star}_{n+1}=0$. Then, note that by definition of a block, $x^{\star}_{\underline{b}-1}<x^{\star}_{\underline{b}}$ and $x^{\star}_{\overline{b}}<x^{\star}_{\overline{b}+1}$, and by complementarity $\lambda^{\star}_{\underline{b}-1} = \lambda^{\star}_{\overline{b}+1} = 0$.

Summing over the elements of $B$ in @eq-stationarity yields (telescopic sum):
$$
\begin{align*}
\sum_{i \in B }w_i (y_i-x^{\star}_i) + \lambda_{\underline{b}-1}^{\star} - \lambda_{\overline{b}+1}^{\star} & = 0 \enspace.
\end{align*}
$$
Hence, noticing that the $x_i^{\star}$  in block $B$ are all equal, we have that:

$$
\begin{align*}
x^{\star}_{\underline{b}} = \ldots = x^{\star}_{\overline{b}} = \frac{\sum_{i \in B} w_i y_i}{\sum_{i' \in B} w_{i'}}
\end{align*}
$$



# {{< fa link title="link" >}} References

::: {#refs}
:::
